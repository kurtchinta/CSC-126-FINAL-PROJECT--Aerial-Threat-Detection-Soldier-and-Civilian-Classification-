{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjJ_1Su8aEln"
      },
      "source": [
        "# Final Project (Group): \"Aerial Threat Detection: Soldier and Civilian Classification Using Drone Vision and Deep Learning\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiotlFNoaPtn"
      },
      "source": [
        "# INSTALL REQUIRED PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc5DqIvLaYOX",
        "outputId": "b280f87b-8ba3-4f92-da9b-a6ab2dc7cbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAll packages installed successfully!\n",
            "Libraries imported and ready for use\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for YOLOv8 training and data augmentation\n",
        "!pip install -q ultralytics albumentations opencv-python-headless\n",
        "\n",
        "# Import essential libraries\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"All packages installed successfully!\")\n",
        "print(\"Libraries imported and ready for use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8f4nzR2agKH"
      },
      "source": [
        "# MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0km52mX_arln",
        "outputId": "ee9bd3aa-d727-42cf-9e7f-a23a69b7ebf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "============================================================\n",
            "STEP 1: Auto-detecting dataset structure...\n",
            "============================================================\n",
            "\n",
            "Searching in: /content/drive/MyDrive/csc-final-project-aerial.v1-csc_dataset.yolov8\n",
            "\n",
            "Contents of csc-final-project-aerial.v1-csc_dataset.yolov8:\n",
            "  README.dataset.txt\n",
            "  README.roboflow.txt\n",
            "  data.yaml\n",
            "  test/ (2 items)\n",
            "  train/ (2 items)\n",
            "  valid/ (2 items)\n",
            "\n",
            "============================================================\n",
            "STEP 2: Detecting YOLO dataset structure...\n",
            "============================================================\n",
            "\n",
            "Found standard YOLOv8 structure:\n",
            "  train/ - 2825 images\n",
            "  valid/ - 120 images\n",
            "  test/ - 52 images\n",
            "\n",
            "============================================================\n",
            "STEP 3: Copying dataset to Colab workspace...\n",
            "============================================================\n",
            "Copied train images: 2825 files\n",
            "Copied train labels: 2825 files\n",
            "Copied valid images: 120 files\n",
            "Copied valid labels: 104 files\n",
            "Copied test images: 52 files\n",
            "Copied test labels: 52 files\n",
            "\n",
            "============================================================\n",
            "SUCCESS: Dataset ready!\n",
            "============================================================\n",
            "Working directory: /content/civilian_soldier_working\n",
            "\n",
            "Final dataset structure:\n",
            "\n",
            "train/\n",
            "  images/ (2825 files)\n",
            "  labels/ (2825 files)\n",
            "\n",
            "val/\n",
            "  images/ (120 files)\n",
            "  labels/ (104 files)\n",
            "\n",
            "test/\n",
            "  images/ (52 files)\n",
            "  labels/ (52 files)\n",
            "\n",
            "============================================================\n",
            "Ready for training!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and Auto-Detect Dataset Structure\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Auto-detecting dataset structure...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Updated path to the correct dataset location\n",
        "GDRIVE_DATASET_PATH = '/content/drive/MyDrive/csc-final-project-aerial.v1-csc_dataset.yolov8'\n",
        "\n",
        "dataset_source = Path(GDRIVE_DATASET_PATH)\n",
        "working_dir = '/content/civilian_soldier_working'\n",
        "\n",
        "print(f\"\\nSearching in: {dataset_source}\")\n",
        "\n",
        "if not dataset_source.exists():\n",
        "    print(f\"\\nERROR: Path not found: {dataset_source}\")\n",
        "    print(\"\\nPlease ensure the dataset folder exists at:\")\n",
        "    print(f\"  {GDRIVE_DATASET_PATH}\")\n",
        "    raise FileNotFoundError(f\"Dataset path not found: {dataset_source}\")\n",
        "\n",
        "# Show what's in the dataset folder\n",
        "print(f\"\\nContents of {dataset_source.name}:\")\n",
        "for item in sorted(dataset_source.iterdir())[:20]:\n",
        "    if item.is_dir():\n",
        "        file_count = len(list(item.iterdir()))\n",
        "        print(f\"  {item.name}/ ({file_count} items)\")\n",
        "    else:\n",
        "        print(f\"  {item.name}\")\n",
        "\n",
        "# Auto-detect dataset structure based on data.yaml format\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"STEP 2: Detecting YOLO dataset structure...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for standard YOLOv8 structure: train/, valid/, test/ folders\n",
        "train_dir = dataset_source / 'train'\n",
        "valid_dir = dataset_source / 'valid'\n",
        "test_dir = dataset_source / 'test'\n",
        "\n",
        "if train_dir.exists() and valid_dir.exists():\n",
        "    print(f\"\\nFound standard YOLOv8 structure:\")\n",
        "    print(f\"  train/ - {len(list((train_dir / 'images').glob('*'))) if (train_dir / 'images').exists() else 0} images\")\n",
        "    print(f\"  valid/ - {len(list((valid_dir / 'images').glob('*'))) if (valid_dir / 'images').exists() else 0} images\")\n",
        "    if test_dir.exists():\n",
        "        print(f\"  test/ - {len(list((test_dir / 'images').glob('*'))) if (test_dir / 'images').exists() else 0} images\")\n",
        "else:\n",
        "    print(\"\\nERROR: Expected train/ and valid/ folders not found!\")\n",
        "    raise ValueError(\"Invalid dataset structure\")\n",
        "\n",
        "# Copy dataset to Colab workspace\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"STEP 3: Copying dataset to Colab workspace...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.makedirs(working_dir, exist_ok=True)\n",
        "working_path = Path(working_dir)\n",
        "\n",
        "# Copy all splits (train, valid/val, test)\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    src_split = dataset_source / split\n",
        "    if src_split.exists():\n",
        "        # Normalize split name (valid -> val)\n",
        "        dest_split = 'val' if split == 'valid' else split\n",
        "\n",
        "        # Copy images\n",
        "        src_images = src_split / 'images'\n",
        "        dst_images = working_path / dest_split / 'images'\n",
        "        if src_images.exists():\n",
        "            dst_images.parent.mkdir(exist_ok=True, parents=True)\n",
        "            if dst_images.exists():\n",
        "                shutil.rmtree(dst_images)\n",
        "            shutil.copytree(src_images, dst_images)\n",
        "            img_count = len(list(dst_images.glob('*')))\n",
        "            print(f\"Copied {split} images: {img_count} files\")\n",
        "\n",
        "        # Copy labels\n",
        "        src_labels = src_split / 'labels'\n",
        "        dst_labels = working_path / dest_split / 'labels'\n",
        "        if src_labels.exists():\n",
        "            dst_labels.parent.mkdir(exist_ok=True, parents=True)\n",
        "            if dst_labels.exists():\n",
        "                shutil.rmtree(dst_labels)\n",
        "            shutil.copytree(src_labels, dst_labels)\n",
        "            lbl_count = len(list(dst_labels.glob('*.txt')))\n",
        "            print(f\"Copied {split} labels: {lbl_count} files\")\n",
        "\n",
        "os.chdir(working_dir)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUCCESS: Dataset ready!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Working directory: {os.getcwd()}\\n\")\n",
        "\n",
        "# Show final structure\n",
        "print(\"Final dataset structure:\")\n",
        "for split in ['train', 'val', 'test']:\n",
        "    split_path = working_path / split\n",
        "    if split_path.exists():\n",
        "        print(f\"\\n{split}/\")\n",
        "        for subdir in sorted(split_path.iterdir()):\n",
        "            if subdir.is_dir():\n",
        "                count = len(list(subdir.glob('*')))\n",
        "                print(f\"  {subdir.name}/ ({count} files)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Ready for training!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZMfol79cEQn"
      },
      "source": [
        "# CREATE DATASET AUGMENTATION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1GdacihcP-2",
        "outputId": "4415071a-3e23-4f76-da8b-b81f038ca1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 10 augmentation pipelines for civilian/soldier detection:\n",
            "Pipeline 1: Rotation, flip, brightness, noise\n",
            "Pipeline 2: Vertical flip, brightness, blur, gamma\n",
            "Pipeline 3: Rotation, color jitter, ISO noise\n",
            "Pipeline 4: Horizontal flip, HSV, CLAHE\n",
            "Pipeline 5: Rotation, brightness, Gaussian blur\n",
            "Pipeline 6: Rotate90, vertical flip, color jitter\n",
            "Pipeline 7: Horizontal flip, Gauss noise, gamma\n",
            "Pipeline 8: Rotation, HSV, blur\n",
            "Pipeline 9: Vertical flip, brightness, ISO noise\n",
            "Pipeline 10: Rotate90, horizontal flip, CLAHE, color jitter\n",
            "This will generate 10x more training data for better person detection!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/augmentations/blur/functional.py:232: UserWarning: blur_limit: Non-zero kernel sizes must be odd. Range (3, 4) automatically adjusted to (3, 5).\n",
            "  result = _ensure_odd_values(result, info.field_name)\n"
          ]
        }
      ],
      "source": [
        "def create_augmentation_pipeline():\n",
        "    \"\"\"Create advanced augmentation pipeline for musical instrument detection images\"\"\"\n",
        "\n",
        "    # Define 10 different augmentation transforms with bbox support\n",
        "    transform_set_1 = A.Compose([\n",
        "        A.RandomRotate90(p=0.7),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.2,\n",
        "            contrast_limit=0.2,\n",
        "            p=0.6\n",
        "        ),\n",
        "        A.GaussNoise(noise_scale_factor=0.1, p=0.4),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_2 = A.Compose([\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.3,\n",
        "            contrast_limit=0.3,\n",
        "            p=0.7\n",
        "        ),\n",
        "        A.Blur(blur_limit=3, p=0.3),\n",
        "        A.RandomGamma(gamma_limit=(80, 120), p=0.4),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_3 = A.Compose([\n",
        "        A.Rotate(limit=15, p=0.6),\n",
        "        A.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05, p=0.5),\n",
        "        A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_4 = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.4),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_5 = A.Compose([\n",
        "        A.Rotate(limit=30, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.6),\n",
        "        A.GaussianBlur(blur_limit=3, sigma_limit=0, p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_6 = A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.VerticalFlip(p=0.4),\n",
        "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.6),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_7 = A.Compose([\n",
        "        A.HorizontalFlip(p=0.6),\n",
        "        A.GaussNoise(noise_scale_factor=0.15, p=0.5),\n",
        "        A.RandomGamma(gamma_limit=(70, 130), p=0.5),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_8 = A.Compose([\n",
        "        A.Rotate(limit=20, p=0.7),\n",
        "        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=15, p=0.6),\n",
        "        A.Blur(blur_limit=4, p=0.4),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_9 = A.Compose([\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.35, contrast_limit=0.35, p=0.6),\n",
        "        A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.15, 0.6), p=0.4),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    transform_set_10 = A.Compose([\n",
        "        A.RandomRotate90(p=0.6),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=0.5),\n",
        "        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.15, hue=0.08, p=0.5),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    return [transform_set_1, transform_set_2, transform_set_3, transform_set_4, transform_set_5,\n",
        "            transform_set_6, transform_set_7, transform_set_8, transform_set_9, transform_set_10]\n",
        "\n",
        "def parse_yolo_annotation(annotation_path):\n",
        "    \"\"\"Parse YOLO format annotation file\"\"\"\n",
        "    bboxes = []\n",
        "    class_labels = []\n",
        "\n",
        "    if annotation_path.exists():\n",
        "        with open(annotation_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    class_id = int(float(parts[0]))  # Convert to int, handle floats like '2.0'\n",
        "                    bbox = [float(x) for x in parts[1:5]]  # x_center, y_center, width, height\n",
        "                    class_labels.append(class_id)\n",
        "                    bboxes.append(bbox)\n",
        "\n",
        "    return bboxes, class_labels\n",
        "\n",
        "def save_yolo_annotation(annotation_path, bboxes, class_labels):\n",
        "    \"\"\"Save YOLO format annotation file\"\"\"\n",
        "    with open(annotation_path, 'w') as f:\n",
        "        for bbox, class_id in zip(bboxes, class_labels):\n",
        "            # Format: class_id x_center y_center width height\n",
        "            f.write(f\"{class_id} {' '.join(map(str, bbox))}\\n\")\n",
        "\n",
        "# Create augmentation pipelines\n",
        "augmentation_pipelines = create_augmentation_pipeline()\n",
        "print(\"Created 10 augmentation pipelines for civilian/soldier detection:\")\n",
        "print(\"Pipeline 1: Rotation, flip, brightness, noise\")\n",
        "print(\"Pipeline 2: Vertical flip, brightness, blur, gamma\")\n",
        "print(\"Pipeline 3: Rotation, color jitter, ISO noise\")\n",
        "print(\"Pipeline 4: Horizontal flip, HSV, CLAHE\")\n",
        "print(\"Pipeline 5: Rotation, brightness, Gaussian blur\")\n",
        "print(\"Pipeline 6: Rotate90, vertical flip, color jitter\")\n",
        "print(\"Pipeline 7: Horizontal flip, Gauss noise, gamma\")\n",
        "print(\"Pipeline 8: Rotation, HSV, blur\")\n",
        "print(\"Pipeline 9: Vertical flip, brightness, ISO noise\")\n",
        "print(\"Pipeline 10: Rotate90, horizontal flip, CLAHE, color jitter\")\n",
        "print(\"This will generate 10x more training data for better person detection!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPj15pRDcfAv"
      },
      "source": [
        "# YOLO CONFIGURATION FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we8HkDtTcgNw",
        "outputId": "d62a7cb4-2005-48f2-b21e-6d78799a7d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ Creating YOLO configuration file...\n",
            "Found data.yaml file - using it!\n",
            "YOLO configuration created successfully!\n",
            "Dataset path: /content/civilian_soldier_working\n",
            "Number of classes: 2\n",
            "Class names: ['civilian', 'soldier']\n",
            "\n",
            "Final Dataset Summary:\n",
            "  train: 2825 images, 2825 labels\n",
            "  val: 120 images, 104 labels\n",
            "  test: 52 images, 52 labels\n",
            "\n",
            "Ready for YOLOv8 training!\n"
          ]
        }
      ],
      "source": [
        "def create_yolo_config():\n",
        "    \"\"\"Create YAML configuration for YOLOv8 training\"\"\"\n",
        "    print(\"âš™ï¸ Creating YOLO configuration file...\")\n",
        "\n",
        "    # Updated to use the correct dataset path\n",
        "    custom_yaml_path = Path(GDRIVE_DATASET_PATH) / 'data.yaml' if 'GDRIVE_DATASET_PATH' in globals() else None\n",
        "\n",
        "    if custom_yaml_path is not None and custom_yaml_path.exists():\n",
        "        print(\"Found data.yaml file - using it!\")\n",
        "\n",
        "        # Read existing data.yaml\n",
        "        with open(custom_yaml_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "\n",
        "        # Update paths to working directory\n",
        "        config['path'] = str(Path.cwd())\n",
        "        config['train'] = 'train/images'\n",
        "        config['val'] = 'val/images'\n",
        "        config['test'] = 'test/images'\n",
        "\n",
        "        # Ensure we have 2 classes (civilian and soldier)\n",
        "        config['nc'] = 2\n",
        "        config['names'] = ['civilian', 'soldier']\n",
        "\n",
        "        # Save updated config as dataset.yaml\n",
        "        with open('dataset.yaml', 'w') as f:\n",
        "            yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "        print(\"YOLO configuration created successfully!\")\n",
        "        print(f\"Dataset path: {config['path']}\")\n",
        "        print(f\"Number of classes: {config['nc']}\")\n",
        "        print(f\"Class names: {config['names']}\")\n",
        "    else:\n",
        "        print(\"data.yaml not found - creating default configuration\")\n",
        "\n",
        "        config = {\n",
        "            'path': str(Path.cwd()),\n",
        "            'train': 'train/images',\n",
        "            'val': 'val/images',\n",
        "            'test': 'test/images',\n",
        "            'nc': 2,\n",
        "            'names': ['civilian', 'soldier']\n",
        "        }\n",
        "\n",
        "        with open('dataset.yaml', 'w') as f:\n",
        "            yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "        print(\"YOLO configuration created successfully!\")\n",
        "        print(f\"Dataset path: {config['path']}\")\n",
        "        print(f\"Number of classes: {config['nc']}\")\n",
        "        print(f\"Class names: {config['names']}\")\n",
        "\n",
        "    print(\"\\nFinal Dataset Summary:\")\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        images_dir = Path(split) / 'images'\n",
        "        labels_dir = Path(split) / 'labels'\n",
        "        if images_dir.exists() and labels_dir.exists():\n",
        "            image_count = len(list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png')))\n",
        "            label_count = len(list(labels_dir.glob('*.txt')))\n",
        "            print(f\"  {split}: {image_count} images, {label_count} labels\")\n",
        "\n",
        "    return config\n",
        "\n",
        "dataset_config = create_yolo_config()\n",
        "\n",
        "print(\"\\nReady for YOLOv8 training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SnKqJ-2c55H"
      },
      "source": [
        "# TRAIN YOLOv8 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZamPeY-cc_pW",
        "outputId": "e822b602-33e9-4399-a729-637ba9b9752a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "ğŸš Starting YOLOv8 training on civilian/soldier aerial detection dataset...\n",
            "Device: NVIDIA A100-SXM4-40GB\n",
            "CUDA Available: True\n",
            "Loading YOLOv8n model...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 362.8MB/s 0.0s\n",
            "Starting training with optimized hyperparameters...\n",
            "Ultralytics 8.3.234 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=custom_aerial_detection, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/civilian_soldier_working/runs/train/custom_aerial_detection, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 127.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 292.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1405.8Â±593.6 MB/s, size: 51.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/civilian_soldier_working/train/labels... 2825 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2825/2825 1.4Kit/s 2.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/civilian_soldier_working/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 582.9Â±457.9 MB/s, size: 41.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/civilian_soldier_working/val/labels... 104 images, 16 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 120/120 1.2Kit/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/civilian_soldier_working/val/labels.cache\n",
            "Plotting labels to /content/civilian_soldier_working/runs/train/custom_aerial_detection/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/civilian_soldier_working/runs/train/custom_aerial_detection\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      1.15G      2.308      2.345      1.542          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 10.0it/s 35.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 4.6it/s 1.7s\n",
            "                   all        120        164      0.623      0.635      0.666      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      1.36G       2.24       1.84      1.547         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 12.6it/s 28.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.2it/s 0.6s\n",
            "                   all        120        164      0.693       0.64      0.672      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      1.37G      2.246      1.742      1.567          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.3it/s 26.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.782      0.537      0.674      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100      1.39G        2.2      1.615      1.537          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.763      0.727      0.802        0.4\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100      1.41G      2.158      1.523      1.497          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.7it/s 0.5s\n",
            "                   all        120        164      0.697      0.658      0.682      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      1.43G      2.131      1.476      1.487          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.618      0.713      0.707      0.359\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      1.44G      2.107      1.441      1.472          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.2it/s 0.5s\n",
            "                   all        120        164      0.732      0.799      0.821      0.425\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      1.46G      2.098       1.39      1.467          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.7it/s 0.5s\n",
            "                   all        120        164      0.776      0.801      0.806      0.394\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/100      1.48G       2.07      1.356      1.444         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164       0.78      0.737      0.811      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/100      1.49G      2.062      1.336      1.435          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.8it/s 25.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.768      0.807      0.837      0.448\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/100      1.51G      2.038      1.304      1.429          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.8it/s 0.5s\n",
            "                   all        120        164      0.853      0.741      0.832      0.435\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/100      1.53G      2.039      1.286      1.409         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.8it/s 25.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.792      0.817      0.859      0.447\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/100      1.54G      1.994      1.258      1.404          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.0it/s 0.5s\n",
            "                   all        120        164      0.847      0.776      0.863      0.437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/100      1.56G      1.983      1.242      1.388         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.811       0.81      0.835      0.442\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/100      1.58G      1.966      1.213      1.371         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.2it/s 0.5s\n",
            "                   all        120        164      0.821      0.823      0.864      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/100       1.6G       1.95      1.203      1.367          0        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.1it/s 0.5s\n",
            "                   all        120        164      0.807      0.762      0.838      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/100      1.61G      1.949      1.183      1.373          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.0it/s 0.5s\n",
            "                   all        120        164      0.836      0.748      0.824      0.434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/100      1.63G      1.938      1.166       1.37          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.796      0.831      0.862      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/100      1.65G      1.934      1.178      1.368         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.7it/s 0.5s\n",
            "                   all        120        164       0.85      0.768       0.87      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/100      1.66G      1.913      1.142      1.357          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.813      0.793      0.866      0.467\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/100      1.68G      1.917      1.146      1.354         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.6it/s 0.5s\n",
            "                   all        120        164      0.819      0.799      0.852      0.457\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/100       1.7G      1.904      1.118      1.342         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.835      0.829      0.868      0.482\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/100      1.72G      1.915      1.143      1.345          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.9it/s 0.5s\n",
            "                   all        120        164      0.845      0.841      0.863      0.484\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/100      1.74G        1.9      1.094      1.328         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.6it/s 0.5s\n",
            "                   all        120        164      0.815      0.886      0.869      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/100      1.75G      1.874      1.088      1.329          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.8it/s 0.5s\n",
            "                   all        120        164      0.866      0.831      0.877       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/100      1.77G       1.86       1.07      1.329         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.8it/s 25.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.808      0.875      0.888      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/100      1.79G      1.861      1.067      1.313         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.5it/s 0.6s\n",
            "                   all        120        164       0.79      0.849      0.857      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/100       1.8G      1.858      1.059      1.313          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.854      0.853      0.848       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/100      1.82G      1.852      1.056      1.312          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.7it/s 0.5s\n",
            "                   all        120        164      0.842      0.783      0.842      0.448\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/100      1.84G      1.847      1.051      1.304          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.9it/s 25.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.8it/s 0.5s\n",
            "                   all        120        164      0.814      0.878      0.906      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/100      1.85G      1.822       1.03      1.292          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.4it/s 0.6s\n",
            "                   all        120        164      0.844      0.799      0.886      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/100      1.87G      1.829      1.029      1.299          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.4it/s 0.6s\n",
            "                   all        120        164      0.852       0.84      0.886      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/100      1.89G      1.816      1.021      1.295         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.0it/s 0.5s\n",
            "                   all        120        164      0.796      0.872      0.883      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/100       1.9G      1.802     0.9964      1.282          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.849      0.841      0.875      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/100      1.92G      1.781     0.9904      1.285          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164       0.84      0.872      0.907      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/100      1.94G      1.792     0.9905      1.276         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.7it/s 0.5s\n",
            "                   all        120        164      0.813      0.854      0.873      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/100      1.96G      1.789     0.9879      1.284          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.1it/s 0.5s\n",
            "                   all        120        164      0.857      0.829      0.882      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/100      1.97G      1.779     0.9789      1.269          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.7it/s 0.5s\n",
            "                   all        120        164      0.839      0.835      0.851      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/100      1.99G      1.777     0.9772       1.27         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.823       0.86      0.862      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/100      2.01G      1.772     0.9784      1.269         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.5it/s 0.6s\n",
            "                   all        120        164      0.834      0.866      0.895      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/100      2.02G      1.751     0.9636      1.264          0        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.854      0.857      0.899      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/100      2.04G      1.742     0.9497      1.253          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.2it/s 0.6s\n",
            "                   all        120        164      0.816      0.892      0.879      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     43/100      2.06G      1.744     0.9472      1.251          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.7it/s 0.5s\n",
            "                   all        120        164      0.829      0.884      0.884      0.514\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     44/100      2.07G      1.729     0.9274      1.246          0        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.802      0.913      0.868      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     45/100      2.09G      1.724     0.9191      1.246          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.856      0.866      0.889      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     46/100      2.11G      1.719     0.9296      1.249          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.8it/s 0.5s\n",
            "                   all        120        164      0.818      0.929      0.913      0.527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     47/100      2.12G      1.737     0.9314      1.246          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.811      0.902      0.888      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     48/100      2.14G      1.715     0.9112      1.243         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.9it/s 0.5s\n",
            "                   all        120        164        0.8      0.939      0.889      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     49/100      2.16G      1.704     0.9055      1.239          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.824      0.927      0.903      0.538\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     50/100      2.18G      1.694     0.9016      1.227          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.7it/s 0.5s\n",
            "                   all        120        164      0.803      0.909      0.899      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     51/100      2.19G      1.705      0.905      1.232          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.843      0.921      0.885      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     52/100      2.21G      1.671     0.8882      1.222          0        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.0it/s 0.5s\n",
            "                   all        120        164      0.829      0.915      0.892      0.519\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     53/100      2.23G      1.677     0.8835      1.222          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.6it/s 0.5s\n",
            "                   all        120        164      0.824      0.878      0.868      0.484\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     54/100      2.25G      1.672     0.8836      1.216         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 16.2it/s 0.5s\n",
            "                   all        120        164      0.831       0.89      0.908      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     55/100      2.26G      1.655      0.872      1.214          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.8it/s 0.5s\n",
            "                   all        120        164      0.849      0.902        0.9      0.524\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     56/100      2.28G      1.655     0.8694      1.213          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164      0.857      0.915      0.881      0.515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     57/100       2.3G      1.656     0.8753      1.215         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.1it/s 0.5s\n",
            "                   all        120        164      0.823      0.902      0.884      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     58/100      2.31G      1.644     0.8625       1.21          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.9it/s 0.5s\n",
            "                   all        120        164      0.833      0.896      0.895      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     59/100      2.33G      1.651      0.868      1.212          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.2it/s 0.5s\n",
            "                   all        120        164      0.842      0.884      0.883       0.52\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     60/100      2.35G      1.626     0.8557      1.206          0        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.8it/s 0.5s\n",
            "                   all        120        164      0.844      0.891      0.887      0.519\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     61/100      2.37G      1.635     0.8545      1.203         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.6it/s 26.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.5it/s 0.5s\n",
            "                   all        120        164       0.83      0.925      0.884       0.52\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     62/100      2.38G      1.624     0.8441      1.194          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.4it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.8it/s 0.5s\n",
            "                   all        120        164      0.831      0.902      0.892      0.513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     63/100       2.4G      1.637     0.8582      1.208          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.5it/s 26.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 14.1it/s 0.6s\n",
            "                   all        120        164      0.816      0.918      0.883      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     64/100      2.42G      1.616     0.8358      1.199         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 354/354 13.7it/s 25.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 15.3it/s 0.5s\n",
            "                   all        120        164      0.819      0.896      0.897      0.518\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 15 epochs. Best results observed at epoch 49, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=15) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "64 epochs completed in 0.480 hours.\n",
            "Optimizer stripped from /content/civilian_soldier_working/runs/train/custom_aerial_detection/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from /content/civilian_soldier_working/runs/train/custom_aerial_detection/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating /content/civilian_soldier_working/runs/train/custom_aerial_detection/weights/best.pt...\n",
            "Ultralytics 8.3.234 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 7.0it/s 1.1s\n",
            "                   all        120        164      0.824      0.927      0.903      0.539\n",
            "              civilian        104        164      0.824      0.927      0.903      0.539\n",
            "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 6.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/civilian_soldier_working/runs/train/custom_aerial_detection\u001b[0m\n",
            "\n",
            "âœ… Training completed successfully!\n",
            "ğŸ“ Training results saved to runs/train/custom_aerial_detection\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "print(\"ğŸš Starting YOLOv8 training on civilian/soldier aerial detection dataset...\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Initialize YOLOv8 model\n",
        "print(\"Loading YOLOv8n model...\")\n",
        "model = YOLO('yolov8n.pt')  # Using nano model for faster training\n",
        "\n",
        "# Start training with optimized parameters\n",
        "print(\"Starting training with optimized hyperparameters...\")\n",
        "\n",
        "results = model.train(\n",
        "    data='dataset.yaml',           # Dataset configuration file\n",
        "    epochs=100,                    # Number of training epochs\n",
        "    patience=15,                   # Early stopping patience\n",
        "    batch=8,                       # Batch size (adjust based on GPU memory)\n",
        "    imgsz=640,                     # Image size for training\n",
        "    device=0 if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    project='runs/train',          # Project directory for saving results\n",
        "    name='custom_aerial_detection',# Experiment name\n",
        "    save=True,                     # Save model checkpoints\n",
        "    save_period=-1,                # Save checkpoint every N epochs (-1 = only last)\n",
        "    cache=False,                   # Cache images for faster training\n",
        "    workers=8,                     # Number of dataloader workers\n",
        "    optimizer='auto',              # Optimizer\n",
        "    verbose=True,                  # Print verbose output\n",
        "    seed=0,                        # Random seed for reproducibility\n",
        "    deterministic=True,            # Deterministic mode\n",
        "    single_cls=False,              # Train as multi-class\n",
        "    rect=False,                    # Rectangular training\n",
        "    cos_lr=False,                  # Cosine learning rate scheduler\n",
        "    close_mosaic=10,               # Disable mosaic augmentation in last N epochs\n",
        "    resume=False,                  # Resume from last checkpoint\n",
        "    amp=True,                      # Automatic Mixed Precision\n",
        "    fraction=1.0,                  # Dataset fraction to use\n",
        "    profile=False,                 # Profile ONNX and TensorRT speeds\n",
        "    freeze=None,                   # Freeze layers\n",
        "    plots=True,                    # Save training plots\n",
        "    val=True                       # Validate after training\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Training completed successfully!\")\n",
        "print(f\"ğŸ“ Training results saved to runs/train/custom_aerial_detection\")\n",
        "\n",
        "# Store results for metrics calculation\n",
        "training_results = results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2tbMQDYr7oO"
      },
      "source": [
        "#UPLOAD RESULTS TO GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F11IPw2sANV",
        "outputId": "95208175-0965-40f4-dacb-c968ab0d0bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â˜ï¸ Uploading results to Google Drive...\n",
            "âœ… Uploaded best.pt (6.0 MB)\n",
            "âœ… Uploaded last.pt (6.0 MB)\n",
            "âœ… Uploaded results.png\n",
            "âœ… Uploaded results.csv\n",
            "âœ… Uploaded confusion_matrix.png\n",
            "âœ… Uploaded confusion_matrix_normalized.png\n",
            "âœ… Uploaded BoxF1_curve.png\n",
            "âœ… Uploaded BoxPR_curve.png\n",
            "âœ… Uploaded BoxP_curve.png\n",
            "âœ… Uploaded BoxR_curve.png\n",
            "âœ… Uploaded labels.jpg\n",
            "âœ… Uploaded val_batch0_labels.jpg\n",
            "âœ… Uploaded val_batch0_pred.jpg\n",
            "âœ… Uploaded val_batch1_labels.jpg\n",
            "âœ… Uploaded val_batch1_pred.jpg\n",
            "âœ… Uploaded val_batch2_labels.jpg\n",
            "âœ… Uploaded val_batch2_pred.jpg\n",
            "âœ… Uploaded train_batch0.jpg\n",
            "âœ… Uploaded train_batch1.jpg\n",
            "âœ… Uploaded train_batch2.jpg\n",
            "âœ… Uploaded args.yaml\n",
            "âœ… Uploaded dataset configuration\n",
            "\n",
            "ğŸ“ All files uploaded to: /content/drive/MyDrive/csc-final-project-aerial.v1-csc_dataset.yolov8/trained_models\n",
            "ğŸ”— Access your trained models directly from Google Drive!\n",
            "\n",
            "ğŸ“¥ Files ready for download:\n",
            "  âœ… runs/train/custom_aerial_detection/weights/best.pt (6.0 MB)\n",
            "  âœ… runs/train/custom_aerial_detection/weights/last.pt (6.0 MB)\n",
            "  âœ… runs/train/custom_aerial_detection/results.png (0.3 MB)\n",
            "  âœ… runs/train/custom_aerial_detection/confusion_matrix.png (0.1 MB)\n",
            "\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘    ğŸš AERIAL THREAT DETECTION TRAINING COMPLETE! ğŸš          â•‘\n",
            "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
            "â•‘                                                              â•‘\n",
            "â•‘  ğŸ¯ Model: YOLOv8n                                          â•‘\n",
            "â•‘  ğŸ“Š Classes: Civilian & Soldier                             â•‘\n",
            "â•‘  ğŸ–¼ï¸  Image Size: 640x640                                    â•‘\n",
            "â•‘  ğŸ”„ Training Complete                                        â•‘\n",
            "â•‘                                                              â•‘\n",
            "â•‘  ğŸ“¥ Downloads Available:                                     â•‘\n",
            "â•‘    â€¢ best.pt (trained model)                                â•‘\n",
            "â•‘    â€¢ last.pt (latest checkpoint)                            â•‘\n",
            "â•‘    â€¢ Training plots and metrics                             â•‘\n",
            "â•‘                                                              â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "ğŸ‰ Congratulations! Your aerial threat detection model is ready!\n",
            "ğŸ¯ Use this model to distinguish soldiers from civilians in aerial drone imagery!\n"
          ]
        }
      ],
      "source": [
        "def upload_results_to_drive():\n",
        "    \"\"\"Upload all training results back to Google Drive\"\"\"\n",
        "\n",
        "    print(\"â˜ï¸ Uploading results to Google Drive...\")\n",
        "\n",
        "    # Define Google Drive destination\n",
        "    drive_destination = '/content/drive/MyDrive/csc-final-project-aerial.v1-csc_dataset.yolov8/trained_models'\n",
        "\n",
        "    # Create destination directory\n",
        "    os.makedirs(drive_destination, exist_ok=True)\n",
        "\n",
        "    # Find the latest training run\n",
        "    runs_dir = Path('runs/train')\n",
        "    if runs_dir.exists():\n",
        "        latest_run = max(runs_dir.glob('custom_aerial_detection*'), key=lambda x: x.stat().st_mtime)\n",
        "        weights_dir = latest_run / 'weights'\n",
        "\n",
        "        if weights_dir.exists():\n",
        "            # Upload model files\n",
        "            model_files = ['best.pt', 'last.pt']\n",
        "\n",
        "            for model_file in model_files:\n",
        "                src_path = weights_dir / model_file\n",
        "                if src_path.exists():\n",
        "                    dst_path = f\"{drive_destination}/{model_file}\"\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "                    size_mb = src_path.stat().st_size / 1024 / 1024\n",
        "                    print(f\"âœ… Uploaded {model_file} ({size_mb:.1f} MB)\")\n",
        "\n",
        "            # Upload training plots and results\n",
        "            result_files = [\n",
        "                'results.png',\n",
        "                'results.csv',\n",
        "                'confusion_matrix.png',\n",
        "                'confusion_matrix_normalized.png',\n",
        "                'F1_curve.png',\n",
        "                'P_curve.png',\n",
        "                'R_curve.png',\n",
        "                'PR_curve.png',\n",
        "                'BoxF1_curve.png',\n",
        "                'BoxPR_curve.png',\n",
        "                'BoxP_curve.png',\n",
        "                'BoxR_curve.png',\n",
        "                'labels.jpg',\n",
        "                'labels_correlogram.jpg',\n",
        "                'val_batch0_labels.jpg',\n",
        "                'val_batch0_pred.jpg',\n",
        "                'val_batch1_labels.jpg',\n",
        "                'val_batch1_pred.jpg',\n",
        "                'val_batch2_labels.jpg',\n",
        "                'val_batch2_pred.jpg',\n",
        "                'train_batch0.jpg',\n",
        "                'train_batch1.jpg',\n",
        "                'train_batch2.jpg',\n",
        "                'args.yaml'\n",
        "            ]\n",
        "\n",
        "            for result_file in result_files:\n",
        "                src_path = latest_run / result_file\n",
        "                if src_path.exists():\n",
        "                    dst_path = f\"{drive_destination}/{result_file}\"\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "                    print(f\"âœ… Uploaded {result_file}\")\n",
        "\n",
        "    # Upload dataset configuration\n",
        "    if Path('dataset.yaml').exists():\n",
        "        dst_path = f\"{drive_destination}/dataset.yaml\"\n",
        "        shutil.copy2('dataset.yaml', dst_path)\n",
        "        print(\"âœ… Uploaded dataset configuration\")\n",
        "\n",
        "    print(f\"\\nğŸ“ All files uploaded to: {drive_destination}\")\n",
        "    print(\"ğŸ”— Access your trained models directly from Google Drive!\")\n",
        "\n",
        "    return drive_destination\n",
        "\n",
        "# Upload results to Google Drive\n",
        "drive_path = upload_results_to_drive()\n",
        "\n",
        "# Prepare download summary\n",
        "print(\"\\nğŸ“¥ Files ready for download:\")\n",
        "\n",
        "files_to_download = [\n",
        "    'runs/train/custom_aerial_detection/weights/best.pt',\n",
        "    'runs/train/custom_aerial_detection/weights/last.pt',\n",
        "    'runs/train/custom_aerial_detection/results.png',\n",
        "    'runs/train/custom_aerial_detection/confusion_matrix.png',\n",
        "]\n",
        "\n",
        "for file_path in files_to_download:\n",
        "    if Path(file_path).exists():\n",
        "        size = Path(file_path).stat().st_size / 1024 / 1024\n",
        "        print(f\"  âœ… {file_path} ({size:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"  âŒ {file_path} (not found)\")\n",
        "\n",
        "# Display final success message\n",
        "print(f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘    ğŸš AERIAL THREAT DETECTION TRAINING COMPLETE! ğŸš          â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                              â•‘\n",
        "â•‘  ğŸ¯ Model: YOLOv8n                                          â•‘\n",
        "â•‘  ğŸ“Š Classes: Civilian & Soldier                             â•‘\n",
        "â•‘  ğŸ–¼ï¸  Image Size: 640x640                                    â•‘\n",
        "â•‘  ğŸ”„ Training Complete                                        â•‘\n",
        "â•‘                                                              â•‘\n",
        "â•‘  ğŸ“¥ Downloads Available:                                     â•‘\n",
        "â•‘    â€¢ best.pt (trained model)                                â•‘\n",
        "â•‘    â€¢ last.pt (latest checkpoint)                            â•‘\n",
        "â•‘    â€¢ Training plots and metrics                             â•‘\n",
        "â•‘                                                              â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "print(\"ğŸ‰ Congratulations! Your aerial threat detection model is ready!\")\n",
        "print(\"ğŸ¯ Use this model to distinguish soldiers from civilians in aerial drone imagery!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_E5y8BhsIro"
      },
      "source": [
        "# DOWNLOAD THE WHOLE LOCAL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oag9V2k8sQgm",
        "outputId": "73bfb631-21ce-4ea9-be7d-c44cd5ca089f"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_554dc4f4-a02e-4a9f-b9b5-3917d62de1cf\", \"civilian_soldier_working.zip\", 179485664)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip archive of the civilian_soldier_working folder\n",
        "shutil.make_archive('/content/civilian_soldier_working', 'zip', '/content/civilian_soldier_working')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/civilian_soldier_working.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
